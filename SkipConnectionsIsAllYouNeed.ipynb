{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hBdlN1bC4xxA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-17 20:04:13.928209: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-17 20:04:13.931748: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-04-17 20:04:13.939711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744913053.952873    3381 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744913053.956506    3381 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1744913053.967400    3381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744913053.967423    3381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744913053.967424    3381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1744913053.967426    3381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-04-17 20:04:13.971442: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "class MobileViT_V3XXS():\n",
        "\n",
        "  def build_model(input_shape,include_top=True,num_classes=5,expansion_factor=2,patch_size=4,**kwargs):\n",
        "\n",
        "    conv_block = partial(keras.layers.Conv2D,filters=16,kernel_size=3,strides=2, activation=keras.activations.swish,padding=\"same\")\n",
        "    \n",
        "\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    x = conv_block( filters=16)(inputs) #  conv_block( x,filters=16)\n",
        "    x = MobileViT_V3XXS.inverted_residual_block(\n",
        "       x=x, expanded_channels=16 *expansion_factor, output_channels=16\n",
        "    )\n",
        "\n",
        "    # Downsampling with MV2 block.\n",
        "    x = MobileViT_V3XXS.inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2\n",
        "    )\n",
        "    x = MobileViT_V3XXS.inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "    x = MobileViT_V3XXS.inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "\n",
        "    # First MV2 -> MobileViT block.\n",
        "    x = MobileViT_V3XXS.inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=64, strides=2\n",
        "    )\n",
        "    x = MobileViT_V3XXS.mobilevit_block_v3(x=x,conv_block=conv_block,patch_size=patch_size, num_blocks=2, projection_dim=64)\n",
        "\n",
        "    # Second MV2 -> MobileViT block.\n",
        "    x = MobileViT_V3XXS.inverted_residual_block(\n",
        "        x, expanded_channels=64 * expansion_factor, output_channels=80, strides=2\n",
        "    )\n",
        "    x = MobileViT_V3XXS.mobilevit_block_v3(x,conv_block=conv_block,patch_size=patch_size, num_blocks=4, projection_dim=80)\n",
        "\n",
        "    # Third MV2 -> MobileViT block.\n",
        "    x = MobileViT_V3XXS.inverted_residual_block(\n",
        "        x, expanded_channels=80 * expansion_factor, output_channels=96, strides=2\n",
        "    )\n",
        "    x = MobileViT_V3XXS.mobilevit_block_v3(x,conv_block=conv_block,patch_size=patch_size, num_blocks=3, projection_dim=96)\n",
        "    x = conv_block( filters=320, kernel_size=1, strides=1)(x)\n",
        "\n",
        "    # Classification head.\n",
        "\n",
        "\n",
        "\n",
        "    if include_top:\n",
        "      x = keras.layers.GlobalAvgPool2D()(x)\n",
        "      outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "    else:\n",
        "      outputs = x\n",
        "\n",
        "\n",
        "    return keras.Model(inputs, outputs,name='ViT3XXS')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def correct_pad(inputs, kernel_size):\n",
        "      img_dim = 2 if keras.backend.image_data_format() == \"channels_first\" else 1\n",
        "      input_size = inputs.shape[img_dim : (img_dim + 2)]\n",
        "      if isinstance(kernel_size, int):\n",
        "          kernel_size = (kernel_size, kernel_size)\n",
        "      if input_size[0] is None:\n",
        "          adjust = (1, 1)\n",
        "      else:\n",
        "          adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
        "      correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
        "      return (\n",
        "          (correct[0] - adjust[0], correct[0]),\n",
        "          (correct[1] - adjust[1], correct[1]),\n",
        "      )\n",
        "\n",
        "\n",
        "  # Reference: https://git.io/JKgtC\n",
        "\n",
        "\n",
        "  def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
        "      m = keras.layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
        "      m = keras.layers.BatchNormalization()(m)\n",
        "      m = keras.activations.swish(m)\n",
        "\n",
        "      if strides == 2:\n",
        "          m = keras.layers.ZeroPadding2D(padding=MobileViT_V3XXS.correct_pad(m, 3))(m)\n",
        "      m = keras.layers.DepthwiseConv2D(\n",
        "          3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n",
        "      )(m)\n",
        "      m = keras.layers.BatchNormalization()(m)\n",
        "      m = keras.activations.swish(m)\n",
        "\n",
        "      m = keras.layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
        "      m = keras.layers.BatchNormalization()(m)\n",
        "\n",
        "      if keras.ops.equal(x.shape[-1], output_channels) and strides == 1:\n",
        "          return keras.layers.Add()([m, x])\n",
        "      return m\n",
        "\n",
        "\n",
        "  # Reference:\n",
        "  # https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
        "\n",
        "\n",
        "  def mlp(x, hidden_units, dropout_rate):\n",
        "      for units in hidden_units:\n",
        "          x = keras.layers.Dense(units, activation=keras.activations.swish)(x)\n",
        "          x = keras.layers.Dropout(dropout_rate)(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "  def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n",
        "      for _ in range(transformer_layers):\n",
        "          # Layer normalization 1.\n",
        "          x1 = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "          # Create a multi-head attention layer.\n",
        "          attention_output = keras.layers.MultiHeadAttention(\n",
        "              num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "          )(x1, x1)\n",
        "          # Skip connection 1.\n",
        "          x2 = keras.layers.Add()([attention_output, x])\n",
        "          # Layer normalization 2.\n",
        "          x3 = keras.layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "          # MLP.\n",
        "          x3 = MobileViT_V3XXS.mlp(\n",
        "              x3,\n",
        "              hidden_units=[x.shape[-1] * 2, x.shape[-1]],\n",
        "              dropout_rate=0.1,\n",
        "          )\n",
        "          # Skip connection 2.\n",
        "          x = keras.layers.Add()([x3, x2])\n",
        "\n",
        "      return x\n",
        "\n",
        "  def mobilevit_block_v3(x,conv_block,patch_size,num_blocks, projection_dim, strides=1):\n",
        "      local_features = keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=strides, activation=keras.activations.swish,padding=\"same\")(x)\n",
        "      local_features =  conv_block(\n",
        "          filters=projection_dim, kernel_size=1, strides=strides\n",
        "      )(local_features)\n",
        "\n",
        "      skip2 = local_features\n",
        "\n",
        "\n",
        "      # Unfold into patches and then pass through Transformers.\n",
        "      num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n",
        "\n",
        "      non_overlapping_patches = keras.layers.Reshape((patch_size, num_patches, projection_dim))(\n",
        "          local_features\n",
        "      )\n",
        "\n",
        "      global_features = MobileViT_V3XXS.transformer_block(\n",
        "          non_overlapping_patches, num_blocks, projection_dim\n",
        "      )\n",
        "\n",
        "      # Fold into conv-like feature-maps.\n",
        "      folded_feature_map = keras.layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n",
        "          global_features\n",
        "      )\n",
        "\n",
        "      # Apply point-wise conv -> concatenate with the input features.\n",
        "      folded_feature_map =  conv_block(\n",
        "          filters=skip2.shape[-1], kernel_size=1, strides=strides\n",
        "      )(folded_feature_map)\n",
        "\n",
        "      local_global_features = keras.layers.Concatenate(axis=-1)([skip2, folded_feature_map])\n",
        "\n",
        "      # Fuse the local and global features using a convoluion layer.\n",
        "      local_global_features =  conv_block( filters=projection_dim, strides=strides)(local_global_features)\n",
        "\n",
        "      return local_global_features+x\n",
        "\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1744913058.858198    3381 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
            "W0000 00:00:1744913058.879714    3381 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "new_mobilevit = MobileViT_V3XXS.build_model((64,64,3),include_top=True,num_classes=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sudo] password for bap: \n",
            "sudo: a password is required\n",
            "^C\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_FrlZhX5NgG"
      },
      "outputs": [],
      "source": [
        "from keras import layers,backend\n",
        "# Values are from table 4.\n",
        "patch_size = 4  # 2x2, for the Transformer blocks.\n",
        "image_size = 64\n",
        "expansion_factor = 2  # expansion factor for the MobileNetV2 blocks.\n",
        "\n",
        "\n",
        "def conv_block(x, filters=16, kernel_size=3, strides=2):\n",
        "    conv_layer = layers.Conv2D(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        activation=keras.activations.swish,\n",
        "        padding=\"same\",\n",
        "    )\n",
        "    return conv_layer(x)\n",
        "\n",
        "\n",
        "# Reference: https://github.com/keras-team/keras/blob/e3858739d178fe16a0c77ce7fab88b0be6dbbdc7/keras/applications/imagenet_utils.py#L413C17-L435\n",
        "\n",
        "\n",
        "def correct_pad(inputs, kernel_size):\n",
        "    img_dim = 2 if backend.image_data_format() == \"channels_first\" else 1\n",
        "    input_size = inputs.shape[img_dim : (img_dim + 2)]\n",
        "    if isinstance(kernel_size, int):\n",
        "        kernel_size = (kernel_size, kernel_size)\n",
        "    if input_size[0] is None:\n",
        "        adjust = (1, 1)\n",
        "    else:\n",
        "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
        "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
        "    return (\n",
        "        (correct[0] - adjust[0], correct[0]),\n",
        "        (correct[1] - adjust[1], correct[1]),\n",
        "    )\n",
        "\n",
        "\n",
        "# Reference: https://git.io/JKgtC\n",
        "\n",
        "\n",
        "def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
        "    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = keras.activations.swish(m)\n",
        "\n",
        "    if strides == 2:\n",
        "        m = layers.ZeroPadding2D(padding=correct_pad(m, 3))(m)\n",
        "    m = layers.DepthwiseConv2D(\n",
        "        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n",
        "    )(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = keras.activations.swish(m)\n",
        "\n",
        "    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "\n",
        "    if keras.ops.equal(x.shape[-1], output_channels) and strides == 1:\n",
        "\n",
        "        return layers.Add()([m, x])\n",
        "    return m\n",
        "\n",
        "\n",
        "# Reference:\n",
        "# https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
        "\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=keras.activations.swish)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(\n",
        "            x3,\n",
        "            hidden_units=[x.shape[-1] * 2, x.shape[-1]],\n",
        "            dropout_rate=0.1,\n",
        "        )\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    return x\n",
        "\n",
        "def mobilevit_block_v1(x, num_blocks, projection_dim, strides=1):\n",
        "    # Local projection with convolutions.\n",
        "    local_features = conv_block(x, filters=projection_dim, strides=strides)\n",
        "    local_features = conv_block(\n",
        "        local_features, filters=projection_dim, kernel_size=1, strides=strides\n",
        "    )\n",
        "\n",
        "    # Unfold into patches and then pass through Transformers.\n",
        "    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n",
        "    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n",
        "        local_features\n",
        "    )\n",
        "    global_features = transformer_block(\n",
        "        non_overlapping_patches, num_blocks, projection_dim\n",
        "    )\n",
        "\n",
        "    # Fold into conv-like feature-maps.\n",
        "    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n",
        "        global_features\n",
        "    )\n",
        "\n",
        "    # Apply point-wise conv -> concatenate with the input features.\n",
        "    folded_feature_map = conv_block(\n",
        "        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n",
        "\n",
        "    # Fuse the local and global features using a convoluion layer.\n",
        "    local_global_features = conv_block(\n",
        "        local_global_features, filters=projection_dim, strides=strides\n",
        "    )\n",
        "\n",
        "    return local_global_features\n",
        "\n",
        "\n",
        "def create_mobilevitV1(num_classes=5):\n",
        "    inputs = keras.Input((image_size, image_size, 3))\n",
        "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Initial conv-stem -> MV2 block.\n",
        "    x = conv_block(x, filters=16)\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=16\n",
        "    )\n",
        "\n",
        "    # Downsampling with MV2 block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "\n",
        "    # First MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2\n",
        "    )\n",
        "    x = mobilevit_block_v1(x, num_blocks=2, projection_dim=64)\n",
        "\n",
        "    # Second MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2\n",
        "    )\n",
        "    x = mobilevit_block_v1(x, num_blocks=4, projection_dim=80)\n",
        "\n",
        "    # Third MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2\n",
        "    )\n",
        "    x = mobilevit_block_v1(x, num_blocks=3, projection_dim=96)\n",
        "    x = conv_block(x, filters=320, kernel_size=1, strides=1)\n",
        "\n",
        "    # Classification head.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk-Jbh3T48vX",
        "outputId": "db8f37b6-0323-4ba9-cae3-9c888e1f33f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coku-aH85AvU"
      },
      "outputs": [],
      "source": [
        "ViT3 = MobileViT_V3XXS((64,64,3),include_top=True,num_classes=10,is_imagenet=True).get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeT2qI7D7SnT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tZQSrVgA5D2_"
      },
      "outputs": [],
      "source": [
        "ViT3Complete= keras.Sequential([keras.Input(shape=(32,32,3)),\n",
        "                                  keras.layers.Resizing(height=64,width=64),\n",
        "                                  new_mobilevit])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkaVMAih-YUr"
      },
      "outputs": [],
      "source": [
        "ViT1 = create_mobilevitV1(num_classes=10)\n",
        "ViT1Complete = keras.Sequential([keras.Input(shape=(32,32,3)),\n",
        "                                  keras.layers.Resizing(height=64,width=64),\n",
        "                                  ViT1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-_TAZK35nwe",
        "outputId": "1cb3c515-9c29-48ce-ce39-80e52e44c14f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 164ms/step - accuracy: 0.3793 - loss: 1.6495 - val_accuracy: 0.1000 - val_loss: 2.7317\n",
            "Epoch 2/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 45ms/step - accuracy: 0.6190 - loss: 1.0570 - val_accuracy: 0.1065 - val_loss: 5.4029\n",
            "Epoch 3/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.7026 - loss: 0.8364 - val_accuracy: 0.2372 - val_loss: 2.7262\n",
            "Epoch 4/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 44ms/step - accuracy: 0.7557 - loss: 0.6819 - val_accuracy: 0.1088 - val_loss: 4.2970\n",
            "Epoch 5/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - accuracy: 0.7983 - loss: 0.5717 - val_accuracy: 0.1930 - val_loss: 3.2019\n",
            "Epoch 6/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.8256 - loss: 0.4874 - val_accuracy: 0.3460 - val_loss: 2.3065\n",
            "Epoch 7/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.8612 - loss: 0.3945 - val_accuracy: 0.1325 - val_loss: 4.5710\n",
            "Epoch 8/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.8791 - loss: 0.3450 - val_accuracy: 0.2200 - val_loss: 3.3261\n",
            "Epoch 9/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.9031 - loss: 0.2757 - val_accuracy: 0.1073 - val_loss: 6.0602\n",
            "Epoch 10/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.9174 - loss: 0.2396 - val_accuracy: 0.1038 - val_loss: 6.3280\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d52344590d0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "early = keras.callbacks.EarlyStopping(patience=4,restore_best_weights=True)\n",
        "ViT1Complete.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),loss='SparseCategoricalCrossentropy',metrics=['accuracy'])\n",
        "ViT1Complete.fit(train_images,train_labels,validation_data=(test_images,test_labels),batch_size=128,epochs=100,callbacks=[early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4z2ZD7G5jK0",
        "outputId": "069acbff-e1c8-45c1-ee33-878466b9aa30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 193ms/step - accuracy: 0.3554 - loss: 1.7163 - val_accuracy: 0.1000 - val_loss: 2.9361\n",
            "Epoch 2/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 45ms/step - accuracy: 0.5913 - loss: 1.1411 - val_accuracy: 0.5885 - val_loss: 1.1577\n",
            "Epoch 3/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - accuracy: 0.6680 - loss: 0.9198 - val_accuracy: 0.6623 - val_loss: 0.9739\n",
            "Epoch 4/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.7218 - loss: 0.7853 - val_accuracy: 0.6809 - val_loss: 0.9212\n",
            "Epoch 5/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - accuracy: 0.7588 - loss: 0.6803 - val_accuracy: 0.6957 - val_loss: 0.8798\n",
            "Epoch 6/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 46ms/step - accuracy: 0.7849 - loss: 0.6087 - val_accuracy: 0.6964 - val_loss: 0.9420\n",
            "Epoch 7/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - accuracy: 0.8103 - loss: 0.5403 - val_accuracy: 0.7272 - val_loss: 0.8256\n",
            "Epoch 8/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.8269 - loss: 0.4862 - val_accuracy: 0.7446 - val_loss: 0.7867\n",
            "Epoch 9/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 44ms/step - accuracy: 0.8483 - loss: 0.4307 - val_accuracy: 0.7479 - val_loss: 0.8013\n",
            "Epoch 10/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.8651 - loss: 0.3804 - val_accuracy: 0.7314 - val_loss: 0.8755\n",
            "Epoch 11/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.8804 - loss: 0.3396 - val_accuracy: 0.7340 - val_loss: 0.9017\n",
            "Epoch 12/100\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - accuracy: 0.8959 - loss: 0.2954 - val_accuracy: 0.7465 - val_loss: 0.9272\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d537ae25710>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "early = keras.callbacks.EarlyStopping(patience=4,restore_best_weights=True)\n",
        "ViT3Complete.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),loss='SparseCategoricalCrossentropy',metrics=['accuracy'])\n",
        "ViT3Complete.fit(train_images,train_labels,validation_data=(test_images,test_labels),batch_size=128,epochs=100,callbacks=[early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmXO11ib5lu4",
        "outputId": "62abc932-f1f9-47f8-db45-cae42dc2c9c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.3505 - loss: 2.3187\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7456 - loss: 0.7831\n",
            "ViT1 accuracy=0.34599998593330383\n",
            "ViT3 accuracy=0.7445999979972839\n"
          ]
        }
      ],
      "source": [
        "acc_vit1=ViT1Complete.evaluate(test_images,test_labels)[1]\n",
        "acc_vit3=ViT3Complete.evaluate(test_images,test_labels)[1]\n",
        "\n",
        "\n",
        "print(\"ViT1 accuracy=\"+str(acc_vit1)+f'\\nViT3 accuracy='+str(acc_vit3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGFZVMbXJg_8",
        "outputId": "d6e02fdf-10ea-4466-e4c4-1fb0bf10df02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.8390 - loss: 0.4859\n",
            "best ViT3 weights accuracy=0.8385999798774719\n"
          ]
        }
      ],
      "source": [
        "ViT3 = MobileViT_V3XXS((64,64,3),include_top=True,num_classes=10,is_imagenet=True).get_model()\n",
        "ViT3.load_weights(\"./best_weights.weights.h5\")\n",
        "ViT3_best_trained= keras.Sequential([keras.Input(shape=(32,32,3)),\n",
        "                                  keras.layers.Resizing(height=64,width=64),\n",
        "                                  ViT3])\n",
        "\n",
        "ViT3_best_trained.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),loss='SparseCategoricalCrossentropy',metrics=['accuracy'])\n",
        "acc_vit3_best=ViT3_best_trained.evaluate(test_images,test_labels)[1]\n",
        "print(\"best ViT3 weights accuracy=\"+str(acc_vit3_best))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
